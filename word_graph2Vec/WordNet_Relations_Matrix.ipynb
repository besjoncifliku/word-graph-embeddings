{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Relation Matrix from Wordnet\n",
        "\n",
        "- Generate a matrix for each word with each other word in the text w1: (5(syn, hyper, hypo, holo, mero), len(vocab))\n",
        "- Initialize all with zeros\n",
        "- For every word of the vocab  -> check the wordnet for a relation (syn, hyper, hypo, holo, mero) w1 w2\n",
        "- mark 1 all relations that this word w2 has with w1\n",
        "- Methods to add relations as extra knowledge:\n",
        "    - add R in loss sum over all relations\n",
        "\n",
        "\n",
        "Reference on the methods used for extraction of relations:\n",
        "- npit (2019) in Github - https://github.com/npit/nlp-semantic-augmentation/tree/jnle\n",
        "- Thibault Cordier & Antoine Tadros (2019) Project: Learning Word Representations by Embedding the WordNet Graph in Github (https://github.com/ShiroCupz/Embedding-WordNet)\n"
      ],
      "metadata": {
        "id": "3BRsJEIEPvve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn, stopwords\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-D3mS2Tg8w",
        "outputId": "5d23337e-52af-4314-d870-f44fd612daa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the data to google cloud in case the drag-drop upload is not working.\n",
        "from google.colab import files\n",
        "dataset_file_dict = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "3wuYYj_ff0St",
        "outputId": "b3574b4a-ca90-4b98-8aec-e0b5f681e028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f20b59f9-7a18-4f2f-b13e-c1980f68bb94\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f20b59f9-7a18-4f2f-b13e-c1980f68bb94\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving small_corpus_graph_no_stopwords.gpickle to small_corpus_graph_no_stopwords.gpickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have already constructed and uploaded the graph in the sciebo. You can use that one. Or you can construct a new one according to the HOWTO"
      ],
      "metadata": {
        "id": "c91HlEgc3Uet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the graph\n",
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "def load_graph(path='/content/corpus_graph.gpickle'):\n",
        "    G = nx.read_gpickle(path)\n",
        "    print('>> Load Graph: ', G)\n",
        "    return G"
      ],
      "metadata": {
        "id": "NnJw_gD1gNZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = load_graph('/content/small_corpus_graph_no_stopwords.gpickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgOqviczh-7K",
        "outputId": "5eb59393-c9d4-4d18-b769-cf4a86579757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Load Graph:  DiGraph with 61167 nodes and 12902 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = list(G.nodes)[:20000]"
      ],
      "metadata": {
        "id": "_5-lW9uJiG4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique words in the corpus\n",
        "def get_nodes(doc):\n",
        "    # Efficient implementation of get_entities\n",
        "    split_text = doc.split()\n",
        "    unique_words = set(split_text)\n",
        "    return unique_words\n",
        "\n",
        "# Get unique words from the Graph\n",
        "def get_graph_nodes(G):\n",
        "    print(G)\n",
        "    return G.nodes"
      ],
      "metadata": {
        "id": "3yjVDwHALL3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = get_graph_nodes(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h712f6FLiopf",
        "outputId": "29d2c797-67d7-4153-c287-58a41b08e365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph with 61167 nodes and 12902 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def checkHolonym(target, context):\n",
        "    '''\n",
        "    Get sysnsets of both words and ten get holonyms of the first sysnset\n",
        "    check if that is in synset of word 1\n",
        "    '''\n",
        "    synsets_word1 = wn.synsets(target)\n",
        "    synsets_word2 = wn.synsets(context)\n",
        "    for synonym in synsets_word2:\n",
        "        holonyms = synonym.member_holonyms() + synonym.substance_holonyms() + synonym.part_holonyms()\n",
        "        for holonym in holonyms:\n",
        "            if holonym in synsets_word1:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def checkSynonym(target, context):\n",
        "    '''\n",
        "    Check synonym between two words target and see if their sysnet intersect\n",
        "    # Maybe also a cosine similarity with a threshold would work\n",
        "    '''\n",
        "    synsets_word1 = wn.synsets(target)\n",
        "    synsets_word2 = wn.synsets(context)\n",
        "    common_synset = set(synsets_word1).intersection(set(synsets_word2))\n",
        "    if len(common_synset) != 0:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def checkMeronym(target, context):\n",
        "    # Meronym is reverse of holonym swap context and target\n",
        "    return checkHolonym(context, target)\n",
        "\n",
        "\n",
        "def checkHypernym(target, context):\n",
        "    '''\n",
        "    Get synset of both words and then get hypernyms for second word\n",
        "    check if hypernym is in the synset of the first word\n",
        "    '''\n",
        "    hyper = lambda s: s.hypernyms()\n",
        "    synsets_word1 = wn.synsets(target)\n",
        "    synsets_word2 = wn.synsets(context)\n",
        "    for synonym in synsets_word2:\n",
        "        for hypernym in synonym.closure(hyper):\n",
        "            if hypernym in synsets_word1:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def checkHyponym(target, context):\n",
        "    # Reverse of hypernym swap target and context words\n",
        "    return checkHypernym(context, target)\n",
        "\n",
        "\n",
        "def checkAntonym(target, context):\n",
        "    '''\n",
        "    Get synset and lemmas and retrieve antonyms\n",
        "    For each antonym get if there is a common\n",
        "    '''\n",
        "    synsets_word1 = wn.synsets(target)\n",
        "    synsets_word2 = wn.synsets(context)\n",
        "    antonyms = []\n",
        "    for synonym in synsets_word1:\n",
        "        for lemma in synonym.lemmas():\n",
        "            antonyms.extend(lemma.antonyms()) # antonyms() method only works on lemmas.\n",
        "    antonym_names = [antonym.name() for antonym in antonyms]\n",
        "    for antonym_name in antonym_names:\n",
        "        synsets_antonym = wn.synsets(antonym_name)\n",
        "        common_meanings = set(synsets_word2).intersection(set(synsets_antonym))\n",
        "        if len(common_meanings) > 0:\n",
        "            return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "hPKPro2fYvED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a relation matrix. For the lab we were only focused on synonymy and hypernymy."
      ],
      "metadata": {
        "id": "hWKrrj8L3u-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def generateRelationMatrix(nodes):\n",
        "\n",
        "    def getRelations(target, nodes):\n",
        "        # print('>> ', target)\n",
        "        # 0: synonyms, 1: hypernyms, 2:hyponyms, 3:meronyms, 4:holonyms 5:antonyms ()\n",
        "        # Return an array (6, len(nodes)) with 1 and 0 where 1 if target related with a word from list of nodes\n",
        "        relations = ['synonyms', 'hypernyms', 'hyponyms', 'meronyms', 'holonyms', 'antonyms']\n",
        "        # We are only focused on hhyper and syn but you can also define other relation\n",
        "        relations = ['synonyms', ]\n",
        "        num_relations = len(relations)\n",
        "        # def a matrix\n",
        "        relationMatrix = np.zeros((num_relations, len(nodes)))\n",
        "        for i, word in enumerate(nodes):\n",
        "            # Check for the relations between a target node and all other nodes\n",
        "            if target == word:\n",
        "                relationMatrix[:, i] = np.hstack(np.ones(num_relations))\n",
        "                # relationMatrix[:, i] = 1\n",
        "                continue\n",
        "            hyper = checkHypernym(target, word)\n",
        "            syn = checkSynonym(target, word)\n",
        "\n",
        "            # We are only focused on hhyper and syn but you can also define other relation\n",
        "            # hypo = checkHyponym(target, word)\n",
        "            # mer = checkMeronym(target, word)\n",
        "            # ant = checkAntonym(target, word)\n",
        "            # holo = checkHolonym(target, word)\n",
        "            # Generate a 0 1 column for wk wn\n",
        "            # rel = np.hstack((hyper, hypo, mer, holo, syn, ant))\n",
        "            rel = np.hstack((syn, hyper))\n",
        "            # rel = hyper\n",
        "            # print(rel, relationMatrix[:, i])\n",
        "            relationMatrix[:, i] = rel\n",
        "        return relationMatrix\n",
        "\n",
        "    wordsRelations = np.zeros((len(nodes), 1, len(nodes)))\n",
        "    for index, word in enumerate(nodes):\n",
        "        wordRelation = getRelations(word, nodes)\n",
        "        wordsRelations[index, :, :] = wordRelation\n",
        "    return wordsRelations"
      ],
      "metadata": {
        "id": "57onBL6oSm6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relMatrix = generateRelationMatrix(nodes)"
      ],
      "metadata": {
        "id": "Nz8F8Tb9hROn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('relationMatrix.npy', relMatrix)"
      ],
      "metadata": {
        "id": "etzROurJqU13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relMatrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cgosNGgiGtx",
        "outputId": "0e32c128-7d95-4573-d3f6-8149b8248133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 1., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 1., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 1., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}