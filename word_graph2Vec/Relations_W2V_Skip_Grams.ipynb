{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Relation Word2Vec with synonymy or hypernymy\n",
        "The implementation os skip gram model might not be the exact one presented in the paper but I tried to do it similarly.\n",
        "\n",
        "Reference for the skip gram model according to a Github implementation:\n",
        "https://github.com/n0obcoder/Skip-Gram_Model-TensorFlow/blob/master/model.py\n",
        "\n",
        "I added relation and modified the loss function.\n",
        "\n",
        "You just neeed to run the notebook inside colab or your own environmnet. I suggest colab as every library is already included there so no need for extra work."
      ],
      "metadata": {
        "id": "c2cHzV12F9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some text data from wikipedia\n",
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "dataset_folder_path = 'content'\n",
        "dataset_filename = 'text8.zip'\n",
        "dataset_name = 'Text8 Dataset'\n",
        "class DLProgress(tqdm):\n",
        "    last_block = 0\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "if not isfile(dataset_filename):\n",
        "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
        "        urlretrieve(\n",
        "            'http://mattmahoney.net/dc/text8.zip',\n",
        "            dataset_filename,\n",
        "            pbar.hook)\n",
        "if not isdir(dataset_folder_path):\n",
        "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nXuGdTihlIq",
        "outputId": "a8b6fe9b-6adf-484b-85d4-c06aaf72914a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Text8 Dataset: 31.4MB [00:34, 905kB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llhjGRgbTdde",
        "outputId": "f69e3889-50f5-4518-baa0-f36bce3f2c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQFfCf-1WNfk"
      },
      "outputs": [],
      "source": [
        "relation_matrix_path = \"/content/gdrive/MyDrive/GloveEmbeddings/relationMatrix_improved.npy\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the data to google cloud in case the drag-drop upload is not working.\n",
        "from google.colab import files\n",
        "dataset_file_dict = files.upload()"
      ],
      "metadata": {
        "id": "BMBdJ9ylgOBf",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "682571f4-964a-445e-d562-7c34bea01935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b671f2e2-e020-4b54-be87-faf1590108b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b671f2e2-e020-4b54-be87-faf1590108b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving corpus_graph.gpickle to corpus_graph.gpickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec with Synonymy or Hypernymy Relations"
      ],
      "metadata": {
        "id": "N-G16Kpz1X_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Preleminaries"
      ],
      "metadata": {
        "id": "9fLxJB_DJKkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import random\n",
        "FILE_PATH = \"text8.zip\""
      ],
      "metadata": {
        "id": "kVhW6jt2fEuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "tf.test.is_gpu_available()\n",
        "DATA_SOURCE = 'gensim' # or 'toy'\n",
        "# DATA_SOURCE = 'toy'\n",
        "MODEL_ID = DATA_SOURCE #'toy'# 'gensim'\n",
        "DISPLAY_BATCH_LOSS = True\n",
        "DISPLAY_EVERY_N_BATCH = 1000\n",
        "SAVE_EVERY_N_EPOCH = 1\n",
        "BATCH_SIZE = 1024*16\n",
        "NUM_EPOCHS = 1\n",
        "CONTEXT_SIZE = 3\n",
        "FRACTION_DATA = 1\n",
        "SUBSAMPLING = True\n",
        "SAMPLING_RATE = 0.001\n",
        "NEGATIVE_SAMPLES = 20 # set it to 0 if you don't want to use negative samplings\n",
        "EMBEDDING_DIM = 100\n",
        "LR = 0.001\n",
        "if FRACTION_DATA == 1:\n",
        "    TEST_WORDS = ['india', 'computer', 'gold', 'football', 'cars', 'war', 'apple', 'music', 'helicopter']\n",
        "    TEST_WORDS_VIZ = ['india', 'asia', 'guitar', 'piano', 'album', 'music', 'war', 'soldiers', 'helicopter']\n",
        "else:\n",
        "    TEST_WORDS = ['human', 'boy', 'office', 'woman']\n",
        "    TEST_WORDS_VIZ = TEST_WORDS\n",
        "\n",
        "PREPROCESSED_DATA_DIR = os.path.join(MODEL_ID, 'preprocessed_data')\n",
        "PREPROCESSED_DATA_PATH = os.path.join(PREPROCESSED_DATA_DIR, 'preprocessed_data_' + MODEL_ID + '_' + str(FRACTION_DATA) + '.pickle')\n",
        "SUMMARY_DIR = os.path.join(MODEL_ID, 'summary')\n",
        "MODEL_DIR = os.path.join(MODEL_ID, 'models')\n",
        "DATA_DICT_PATH = os.path.join(MODEL_ID, 'data_dict.pickle')"
      ],
      "metadata": {
        "id": "viAAftSGmyRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98149b45-b90b-42b7-fd40-e1fceb1ce9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-58f0bec726bd>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function"
      ],
      "metadata": {
        "id": "ZJrdzcl2Jn3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def q():\n",
        "    sys.exit()\n",
        "\n",
        "def count_parameters(model):\n",
        "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return num_parameters/1e6 # in terms of millions\n",
        "\n",
        "# TEST\n",
        "def nearest_word(inp, emb, top = 5, debug = False):\n",
        "    euclidean_dis = np.linalg.norm(inp - emb, axis = 1)\n",
        "    emb_ranking = np.argsort(euclidean_dis)\n",
        "    emb_ranking_distances = euclidean_dis[emb_ranking[:top]]\n",
        "    emb_ranking_top = emb_ranking[:top]\n",
        "    euclidean_dis_top = euclidean_dis[emb_ranking_top]\n",
        "    if debug:\n",
        "        print('euclidean_dis: ', euclidean_dis)\n",
        "        print('emb_ranking: ', emb_ranking)\n",
        "        print(f'top {top} embeddings are: {emb_ranking[:top]} with respective distances\\n {euclidean_dis_top}')\n",
        "    return emb_ranking_top, euclidean_dis_top"
      ],
      "metadata": {
        "id": "JOVe0JB5n4X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle, pdb\n",
        "import tensorflow as tf\n",
        "\n",
        "def print_nearest_words_tf(model, test_words, word_to_ix, ix_to_word, top = 5):\n",
        "    emb_matrix = model.embeddings_input(np.array(range(len(word_to_ix))))\n",
        "    print('type(emb_matrix): ', type(emb_matrix))\n",
        "    nearest_words_dict = {}\n",
        "    print('==============================================')\n",
        "    for t_w in test_words:\n",
        "        inp_emb = emb_matrix[word_to_ix[t_w], :]\n",
        "        emb_ranking_top, _ = nearest_word(inp_emb, emb_matrix, top = top+1)\n",
        "        print(t_w.ljust(10), ' | ', ', '.join([ix_to_word[i] for i in emb_ranking_top[1:]]))\n",
        "    return nearest_words_dict"
      ],
      "metadata": {
        "id": "F6JZjdQ6n04U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "N4mcuJifJq6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import os, glob, cv2, sys, pdb, random, time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "class original_wordGraph2vec_dataset(Sequence):\n",
        "    def __init__(self, DATA_SOURCE, BATCH_SIZE, CONTEXT_SIZE, FRACTION_DATA, SUBSAMPLING, SAMPLING_RATE):\n",
        "        print(\"Parsing text and loading training data...\")\n",
        "        vocab, word_to_ix, ix_to_word, training_data = self.load_data(DATA_SOURCE, CONTEXT_SIZE, FRACTION_DATA, SUBSAMPLING, SAMPLING_RATE)\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.vocab = vocab\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.ix_to_word = ix_to_word\n",
        "        # training_data is a list of list of 2 indices\n",
        "        self.data = np.array(training_data, dtype = np.uint64)\n",
        "        self.indexes = np.arange(self.data.shape[0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Generate data\n",
        "        x = self.data[indexes, 0]\n",
        "        y = self.data[indexes, 1]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(len(self.data)/self.batch_size)\n",
        "\n",
        "    def gather_training_data(self, split_text, word_to_ix, ix_to_word, context_size):\n",
        "        training_data = []\n",
        "        all_vocab_indices = list(range(len(word_to_ix)))\n",
        "        #for each sentence\n",
        "        print('preparing training data (x, y, graphx graphy)...')\n",
        "        for sentence in tqdm(split_text):\n",
        "            indices = [word_to_ix[word] for word in sentence]\n",
        "            context_words = []\n",
        "            #for each word treated as center word\n",
        "            for center_word_pos in range(len(indices)):\n",
        "                #for each window  position\n",
        "                for w in range(-context_size, context_size+1):\n",
        "                    context_word_pos = center_word_pos + w\n",
        "                    #make sure we dont jump out of the sentence\n",
        "                    if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                        continue\n",
        "                    context_word_idx = indices[context_word_pos]\n",
        "                    center_word_idx  = indices[center_word_pos]\n",
        "                    if center_word_idx == context_word_idx: # same words might be present in the close vicinity of each other. we want to avoid such cases\n",
        "                        continue\n",
        "                    training_data.append([center_word_idx, context_word_idx])\n",
        "        return training_data\n",
        "\n",
        "    def load_data(self, data_source, context_size, fraction_data, subsampling, sampling_rate):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        if data_source == 'toy':\n",
        "            sents = [\n",
        "                    'word1 word2 word3 word4 word5',\n",
        "                    'word6 word7 word8 word9 word10',\n",
        "                    'word11 word12 word13 word14 word15'\n",
        "                    ]\n",
        "            # sents = ['word6 word7 word8 word9 word10', 'word1 word1 word1 word2 word2 word3 word4 word5', 'word11 word12 word13 word14 word15']\n",
        "        elif data_source == 'gensim':\n",
        "            import gensim.downloader as api\n",
        "            dataset = api.load(\"text8\")\n",
        "            data = [d for d in dataset][:int(fraction_data*len([d_ for d_ in dataset]))]\n",
        "            print(f'fraction of data taken: {fraction_data}/1')\n",
        "            sents = []\n",
        "            print('forming sentences by joining tokenized words...')\n",
        "            for d in tqdm(data):\n",
        "                sents.append(' '.join(d))\n",
        "        sent_list_tokenized = [word_tokenize(s) for s in sents]\n",
        "        print('len(sent_list_tokenized): ', len(sent_list_tokenized))\n",
        "        # remove the stopwords\n",
        "        sent_list_tokenized_filtered = []\n",
        "        print('lemmatizing and removing stopwords...')\n",
        "        for s in tqdm(sent_list_tokenized):\n",
        "            sent_list_tokenized_filtered.append([w for w in s if w not in stop_words])\n",
        "        sent_list_tokenized_filtered, vocab, word_to_ix, ix_to_word = self.gather_word_freqs(sent_list_tokenized_filtered, subsampling, sampling_rate)\n",
        "        training_data = self.gather_training_data(sent_list_tokenized_filtered, word_to_ix, ix_to_word, context_size)\n",
        "        return vocab, word_to_ix, ix_to_word, training_data\n",
        "\n",
        "    def gather_word_freqs(self, split_text, subsampling, sampling_rate): #here split_text is sent_list\n",
        "        vocab = {}\n",
        "        ix_to_word = {}\n",
        "        word_to_ix = {}\n",
        "        total = 0.0\n",
        "        print('building vocab...')\n",
        "        for word_tokens in tqdm(split_text):\n",
        "            for word in word_tokens: #for every word in the word list(split_text), which might occur multiple times\n",
        "                if word not in vocab: #only new words allowed\n",
        "                    vocab[word] = 0\n",
        "                    ix_to_word[len(word_to_ix)] = word\n",
        "                    word_to_ix[word] = len(word_to_ix)\n",
        "                vocab[word] += 1.0 #count of the word stored in a dict\n",
        "                total += 1.0 #total number of words in the word_list(split_text)\n",
        "        print('\\nsubsampling: ', subsampling)\n",
        "        if subsampling:\n",
        "            print('performing subsampling...')\n",
        "            for sent in tqdm(split_text):\n",
        "                word_tokens = sent\n",
        "                # print('word_tokens: ', word_tokens)\n",
        "                # print('len(word_tokens): ', len(word_tokens), '\\n')\n",
        "                for i , word in enumerate(word_tokens):\n",
        "                    # print(i, word_tokens[i])\n",
        "                    frac = vocab[word]/total\n",
        "                    prob = 1 - np.sqrt(sampling_rate/frac)\n",
        "                    sampling = np.random.sample()\n",
        "                    #print(sampling, prob)\n",
        "                    if (sampling < prob):\n",
        "                        # print('freq: ', vocab[word_tokens[i]])\n",
        "                        del word_tokens[i]\n",
        "                        i -= 1\n",
        "        return split_text, vocab, word_to_ix, ix_to_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luvRwKSYm8Nj",
        "outputId": "5c043a60-3c1d-472f-bd27-c4034f23e0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relation W2Vec"
      ],
      "metadata": {
        "id": "ZDeuCzwxGsgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_synonym_relation_matrix(path='/content/relationMatrix.npy'):\n",
        "    R = np.load(path)\n",
        "    return R[:, 1, :]\n",
        "\n",
        "def get_hypernym_relation_matrix(path='/content/relationMatrix.npy'):\n",
        "    R = np.load(path)\n",
        "    return R[:, 0, :]"
      ],
      "metadata": {
        "id": "Au9jBct9Hre3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R = get_synonym_relation_matrix(relation_matrix_path)\n",
        "R = get_hypernym_relation_matrix(relation_matrix_path)"
      ],
      "metadata": {
        "id": "Bu9B6_MCHzz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get relation for a specific word in a batch of words\n",
        "def get_relations_between_words(R, target_idx, context_idx):\n",
        "    rel = np.zeros((target_idx.shape[0], 1))\n",
        "    t = R.shape[0]\n",
        "    c = R.shape[1]\n",
        "    for i, target in enumerate(target_idx):\n",
        "        if target < t and context_idx[i] < c:\n",
        "            rel[i] = R[target, context_idx[i]]\n",
        "    return rel"
      ],
      "metadata": {
        "id": "s5Ydbt17Os4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(R.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjRw5bnBIYxc",
        "outputId": "47a812ea-e2b9-4288-cf4a-bddab01a0467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array([5, 6, 7, 8])\n",
        "# Test multiplication in both axis\n",
        "z = np.prod((x, y), axis=0)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt-eLcbSResV",
        "outputId": "d277b11c-7db1-4444-df2c-5f8fc3d8a281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5 12 21 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Loss function"
      ],
      "metadata": {
        "id": "QdWjKIcMJwwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class relations_word2vec_tf(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, embedding_size, vocab_size, noise_dist = None, negative_samples = 0):\n",
        "        super(relations_word2vec_tf, self).__init__()\n",
        "        self.embeddings_input   = tf.keras.layers.Embedding(vocab_size, embedding_size, embeddings_initializer='uniform', mask_zero=False, name=\"w2v_embedding\")\n",
        "        self.embeddings_context = tf.keras.layers.Embedding(vocab_size, embedding_size, embeddings_initializer='uniform', mask_zero=False)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples = negative_samples\n",
        "        self.noise_dist = noise_dist\n",
        "\n",
        "    def call(self, input_word, context_word, relations):\n",
        "        debug = not True\n",
        "        if debug:print('input_word  : ', input_word.shape)\n",
        "        # print('input_word  : ', input_word.shape)\n",
        "        if debug:print('context_word: ', context_word.shape)\n",
        "        # print('context_word: ', context_word.shape)\n",
        "        ##### computing out loss #####\n",
        "        emb_input = self.embeddings_input(input_word)\n",
        "        if debug:print('emb_input: ', emb_input.shape)\n",
        "        emb_context = self.embeddings_context(context_word)\n",
        "        if debug:print('emb_context: ', emb_context.shape)\n",
        "\n",
        "        # Relation synonymy or hypernymy relations\n",
        "        emb_product = tf.keras.layers.dot([emb_input, emb_context], axes = (1, 1))\n",
        "        # Apply the knowledge-regularized method\n",
        "        emb_product = emb_product + tf.math.reduce_prod([relations, emb_product], axis=0)\n",
        "\n",
        "        if debug:print('emb_product.shape: ', emb_product.shape)\n",
        "        out_loss = tf.squeeze(tf.math.log_sigmoid(emb_product), axis = 1)\n",
        "        if debug:print('out_loss.shape: ', out_loss.shape)\n",
        "        return tf.reduce_mean(tf.math.negative(out_loss))\n",
        "    def this(self):\n",
        "        print('testing this!')"
      ],
      "metadata": {
        "id": "R0uVzKxv4EX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download embedding into files"
      ],
      "metadata": {
        "id": "erft_ocNG3xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "def download_embedding_file(word2vec, vocab, title=''):\n",
        "    weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "\n",
        "    # Create and save the vectors and metadata file.\n",
        "    out_v = io.open(f'vectors_{title}.tsv', 'w', encoding='utf-8')\n",
        "    embeddings_file = io.open(f'embeddings_{title}.tsv', 'w', encoding='utf-8')\n",
        "    out_m = io.open(f'metadata_{title}.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "    for index, word in enumerate(vocab):\n",
        "        if index == 0:\n",
        "            continue  # skip 0, it's padding.\n",
        "        vec = weights[index]\n",
        "        embeddings_file.write(word + '\\t' + '\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "        out_m.write(word + \"\\n\")\n",
        "    out_v.close()\n",
        "    out_m.close()\n",
        "    embeddings_file.close()\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(f'vectors_{title}.tsv')\n",
        "        files.download(f'embeddings_{title}.tsv')\n",
        "        files.download(f'metadata_{title}.tsv')\n",
        "    except Exception as e:\n",
        "        print('Error while downloading the file: ', e)"
      ],
      "metadata": {
        "id": "lrH4hC50DaBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "OwzRd6R1J7vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from tqdm import tqdm\n",
        "# from tqdm import tqdm_gui\n",
        "import matplotlib\n",
        "# matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys, pdb, os, shutil, pickle\n",
        "from pprint import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print('gpus: ', gpus)\n",
        "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "# remove MODEL_DIR if it exists\n",
        "if os.path.exists(MODEL_DIR):\n",
        "    shutil.rmtree(MODEL_DIR)\n",
        "# create fresh MODEL_DIR\n",
        "os.makedirs(MODEL_DIR)\n",
        "write_summary = True\n",
        "if write_summary:\n",
        "    # SUMMARY_DIR is the path of the directory where the tensorboard SummaryWriter files are written\n",
        "    # the directory is removed, if it already exists\n",
        "    if os.path.exists(SUMMARY_DIR):\n",
        "        shutil.rmtree(SUMMARY_DIR)\n",
        "    # os.makedirs(SUMMARY_DIR)\n",
        "    train_summary_writer = tf.summary.create_file_writer(SUMMARY_DIR)\n",
        "    summary_counter = 0\n",
        "\n",
        "# make training data and dataloader\n",
        "if not os.path.exists(PREPROCESSED_DATA_PATH):\n",
        "    train_dataset = original_wordGraph2vec_dataset(DATA_SOURCE, BATCH_SIZE, CONTEXT_SIZE, FRACTION_DATA, SUBSAMPLING, SAMPLING_RATE)\n",
        "    if not os.path.exists(PREPROCESSED_DATA_DIR):\n",
        "        os.makedirs(PREPROCESSED_DATA_DIR)\n",
        "    # pickle dump\n",
        "    print('\\ndumping pickle...')\n",
        "    outfile = open(PREPROCESSED_DATA_PATH,'wb')\n",
        "    pickle.dump(train_dataset, outfile)\n",
        "    outfile.close()\n",
        "    print('pickle dumped\\n')\n",
        "else:\n",
        "    # pickle load\n",
        "    print('\\nloading pickle...')\n",
        "    infile = open(PREPROCESSED_DATA_PATH,'rb')\n",
        "    train_dataset = pickle.load(infile)\n",
        "    train_dataset.batch_size = BATCH_SIZE\n",
        "    infile.close()\n",
        "    print('pickle loaded\\n')\n",
        "print('len(train_dataset): ', len(train_dataset))\n",
        "vocab = train_dataset.vocab\n",
        "word_to_ix = train_dataset.word_to_ix\n",
        "ix_to_word = train_dataset.ix_to_word\n",
        "\n",
        "# saving some data information in a pickle file, to be used later while inference/testing\n",
        "# dump data_dict pickle\n",
        "# OVERRITING THE OLDER PICKLE FILE\n",
        "print('\\ndumping data_dict pickle...')\n",
        "outfile = open(DATA_DICT_PATH,'wb')\n",
        "data_dict = {\n",
        "    'vocab': vocab,\n",
        "    'word_to_ix': word_to_ix,\n",
        "    'ix_to_word': ix_to_word\n",
        "    }\n",
        "pickle.dump(data_dict, outfile)\n",
        "outfile.close()\n",
        "print('pickle dumped\\n')\n",
        "print('len(vocab): ', len(vocab), '\\n')\n",
        "\n",
        "# make noise distribution to sample negative examples from\n",
        "word_freqs = np.array(list(vocab.values()))\n",
        "unigram_dist = word_freqs/sum(word_freqs)\n",
        "noise_dist = unigram_dist**(0.75)/np.sum(unigram_dist**(0.75))\n",
        "\n",
        "# Vanilla Skip-Grams : NEGATIVE_SAMPLES = 0\n",
        "NEGATIVE_SAMPLES = 0\n",
        "model = relations_word2vec_tf(EMBEDDING_DIM, len(vocab), noise_dist, NEGATIVE_SAMPLES)\n",
        "optimizer = tf.keras.optimizers.Adam(LR)\n",
        "print('TRAINING...')\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "    print('\\n===== EPOCH {}/{} ====='.format(epoch + 1, NUM_EPOCHS))\n",
        "    # For orginial wg2vec model\n",
        "    for batch_idx, (x, y) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            relations = get_relations_between_words(R, x, y)\n",
        "            loss_value = model(x, y, relations)\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        print('batch# ' + str(batch_idx+1).zfill(len(str(len(train_dataset)))) + '/' + str(len(train_dataset)) + ' | Loss: ' + str(round(loss_value.numpy(), 5)), end = '\\r')\n",
        "        if batch_idx%DISPLAY_EVERY_N_BATCH == 0 and DISPLAY_BATCH_LOSS:\n",
        "            print(f'Batch: {batch_idx+1}/{len(train_dataset)}, Loss: {loss_value}')\n",
        "            # show 5 closest words to some test words\n",
        "            print_nearest_words_tf(model, TEST_WORDS, word_to_ix, ix_to_word, top = 5)\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', loss_value.numpy(), step=summary_counter)\n",
        "            summary_counter += 1\n",
        "    # write embeddings every SAVE_EVERY_N_EPOCH epoch\n",
        "    if epoch%SAVE_EVERY_N_EPOCH == 0:\n",
        "        model.save_weights('{}/model{}'.format(MODEL_DIR, epoch))\n",
        "download_embedding_file(model, vocab, 'original_graph_wg2v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m8OyzCQrnas0",
        "outputId": "0a63d73e-df9d-4ed4-9c0b-891834bdb847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpus:  []\n",
            "Parsing text and loading training data...\n",
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
            "fraction of data taken: 1/1\n",
            "forming sentences by joining tokenized words...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1701/1701 [00:00<00:00, 3957.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(sent_list_tokenized):  1701\n",
            "lemmatizing and removing stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1701/1701 [00:02<00:00, 619.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building vocab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1701/1701 [00:04<00:00, 394.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "subsampling:  True\n",
            "performing subsampling...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1701/1701 [00:23<00:00, 72.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing training data (x, y, graphx graphy)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1701/1701 [01:56<00:00, 14.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dumping pickle...\n",
            "pickle dumped\n",
            "\n",
            "len(train_dataset):  3581\n",
            "\n",
            "dumping data_dict pickle...\n",
            "pickle dumped\n",
            "\n",
            "len(vocab):  253697 \n",
            "\n",
            "TRAINING...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== EPOCH 1/1 =====\n",
            "Batch: 1/3581, Loss: 0.6931406855583191\n",
            "type(emb_matrix):  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "==============================================\n",
            "india       |  dined, ratkaistiin, yoriie, frontline, dovrebanen\n",
            "computer    |  keesh, arrangements, wickens, chignon, precarious\n",
            "gold        |  necromonicon, machrie, poyet, casseipeia, abridge\n",
            "football    |  shaolinquan, sparkled, smithee, launder, failure\n",
            "cars        |  ampsanctus, geographies, hulule, kontinuit, finals\n",
            "war         |  aventis, slovak, pertinentis, xanthophylls, motions\n",
            "apple       |  wetton, vbs, visited, greenie, shirazi\n",
            "music       |  cari, qwest, ratiara, bastiaan, groping\n",
            "helicopter  |  judith, bambuti, vinikour, andreou, murbles\n",
            "Batch: 1001/3581, Loss: 0.13293562829494476\n",
            "type(emb_matrix):  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "==============================================\n",
            "india       |  player, joseph, health, issue, focus\n",
            "computer    |  r, collection, children, events, returned\n",
            "gold        |  read, theories, contact, ordered, fixed\n",
            "football    |  reign, hear, bar, cutting, collapse\n",
            "cars        |  irrigated, customer, finals, interpreter, pi\n",
            "war         |  area, trade, started, traditional, british\n",
            "apple       |  plotting, boosting, millimetres, epistemological, conveying\n",
            "music       |  ever, went, associated, leading, economy\n",
            "helicopter  |  judith, collector, derogatory, lb, rio\n",
            "Batch: 2001/3581, Loss: 0.0494815930724144\n",
            "type(emb_matrix):  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "==============================================\n",
            "india       |  mountain, distinguished, health, joseph, museum\n",
            "computer    |  collection, trade, located, events, total\n",
            "gold        |  island, read, double, theories, population\n",
            "football    |  reign, affairs, unfortunately, household, collapse\n",
            "cars        |  pi, percussion, agencies, scotia, customer\n",
            "war         |  area, members, christian, went, derived\n",
            "apple       |  unforeseen, godfrey, bundle, corey, flares\n",
            "music       |  followed, born, leading, examples, law\n",
            "helicopter  |  judith, finno, fahrenheit, heavens, kilograms\n",
            "Batch: 3001/3581, Loss: 0.011850917711853981\n",
            "type(emb_matrix):  <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "==============================================\n",
            "india       |  joseph, china, woman, married, biography\n",
            "computer    |  army, empire, letter, sold, code\n",
            "gold        |  island, population, joseph, highest, play\n",
            "football    |  reign, kennedy, publishers, collapse, bar\n",
            "cars        |  hometown, xi, scotia, interpreter, percussion\n",
            "war         |  members, received, house, p, next\n",
            "apple       |  stella, mughal, bundle, archduke, shogunate\n",
            "music       |  thomas, went, examples, law, forces\n",
            "helicopter  |  judith, collector, jose, derogatory, whip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [51:32<00:00, 3092.89s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6503226d-a4f7-44af-ad47-c8df09bdfa96\", \"vectors_original_graph_wg2v.tsv\", 306028856)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_93609aa9-f2b4-444d-b2b8-a730d6bb7b90\", \"embeddings_original_graph_wg2v.tsv\", 308265461)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_77c5e2aa-7de4-4a46-b9f9-41c17d646cfd\", \"metadata_original_graph_wg2v.tsv\", 2236605)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}