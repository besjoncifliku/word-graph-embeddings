{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN1ZbIizeyMu"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTiP9PfVfMIU"
      },
      "source": [
        "raw_text = \"\"\"\n",
        "If I'm a bad person, you don't like me\n",
        "Well, I guess I'll make my own way\n",
        "It's a circle, a mean cycle\n",
        "I can't excite you anymore\n",
        "Where's your gavel? Your jury?\n",
        "What's my offense this time?\n",
        "You're not a judge, but if you're gonna judge me\n",
        "Well, sentence me to another life\n",
        "Don't wanna hear your sad songs\n",
        "I don't wanna feel your pain\n",
        "When you swear it's all my fault\n",
        "'Cause you know we're not the same\n",
        "We're not the same\n",
        "Oh, we're not the same\n",
        "Yeah, the friends who stuck together\n",
        "We wrote our names in blood\n",
        "But I guess you can't accept that the change is good\n",
        "It's good, it's good\n",
        "Well, you treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "You treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "Ignorance is your new best friend\n",
        "Ignorance is your new best friend\n",
        "This is the best thing that could have happened\n",
        "Any longer and I wouldn't have made it\n",
        "It's not a war, no, it's not a rapture\n",
        "I'm just a person, but you can't take it\n",
        "The same tricks that, that once fooled me\n",
        "They won't get you anywhere\n",
        "I'm not the same kid from your memory\n",
        "Well, now I can fend for myself\n",
        "Don't wanna hear your sad songs\n",
        "I don't wanna feel your pain\n",
        "When you swear it's all my fault\n",
        "'Cause you know we're not the same\n",
        "We're not the same\n",
        "Oh, we're not the same\n",
        "Yeah, the friends who stuck together\n",
        "We wrote our names in blood\n",
        "But I guess you can't accept that the change is good\n",
        "It's good, it's good\n",
        "Well, you treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "You treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "Ignorance is your new best friend\n",
        "Ignorance is your new best friend\n",
        "Ignorance is your new best friend\n",
        "Ignorance is your new best friend\n",
        "Well, you treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "You treat me just like another stranger\n",
        "Well, it's nice to meet you, sir\n",
        "I guess I'll go\n",
        "I'd best be on my way out\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFv1ROTzfX8T"
      },
      "source": [
        "def preprocess(text):\n",
        "\n",
        "    # Replace punctuation with tokens so we can use them in our model\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' <PERIOD> ')\n",
        "    text = text.replace(',', ' <COMMA> ')\n",
        "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
        "    text = text.replace(';', ' <SEMICOLON> ')\n",
        "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
        "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
        "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
        "    text = text.replace('--', ' <HYPHENS> ')\n",
        "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
        "    text = text.replace(':', ' <COLON> ')\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove all words with  5 or fewer occurences\n",
        "    word_counts = Counter(words)\n",
        "    trimmed_words = [word for word in words if word_counts[word] > 1]\n",
        "\n",
        "    return trimmed_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVw2kEJUfbLk",
        "outputId": "8c664710-e883-4a2c-ace2-f125c8f5099a"
      },
      "source": [
        "# get list of words\n",
        "words = preprocess(raw_text)\n",
        "print(words[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['if', \"i'm\", 'a', 'person', '<COMMA>', 'you', \"don't\", 'like', 'me', 'well', '<COMMA>', 'i', 'guess', \"i'll\", 'my', 'way', \"it's\", 'a', '<COMMA>', 'a', 'i', \"can't\", 'you', 'your', '<QUESTION_MARK>', 'your', '<QUESTION_MARK>', 'my', 'this', '<QUESTION_MARK>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOkJzOaDgAos",
        "outputId": "0b79b095-53f9-477b-b0f1-08e4f9a36a15"
      },
      "source": [
        "# print some stats about this word data\n",
        "print(\"Total words in text: {}\".format(len(words)))\n",
        "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in text: 420\n",
            "Unique words: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSqud2-rgEYm"
      },
      "source": [
        "def create_lookup_tables(words):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param words: Input list of words\n",
        "    :return: A tuple of dicts.  The first dict....\n",
        "    \"\"\"\n",
        "    word_counts = Counter(words)\n",
        "    # sorting the words from most to least frequent in text occurrence\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    # create int_to_vocab dictionaries\n",
        "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYzG5fEBgGzq",
        "outputId": "2f61d2be-89be-49ae-d4de-b9f5243cddb4"
      },
      "source": [
        "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
        "int_words = [vocab_to_int[word] for word in words]\n",
        "\n",
        "print(int_words[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[43, 41, 14, 44, 0, 1, 36, 15, 8, 7, 0, 3, 11, 16, 9, 17, 2, 14, 0, 14, 3, 38, 1, 4, 42, 4, 42, 9, 45, 42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otMZ_M0-g6Z6",
        "outputId": "8b7aee24-94c0-4694-9f39-e71dc304a5d4"
      },
      "source": [
        "threshold = 1e-5\n",
        "word_counts = Counter(int_words)\n",
        "\n",
        "total_count = len(int_words)\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "\n",
        "# discard some frequent words, according to the subsampling equation\n",
        "# create a new list of words for training\n",
        "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "print(train_words[:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[55, 62, 11, 51, 59, 62, 63, 17, 11, 9, 20]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr2LAQD0gLHg"
      },
      "source": [
        "def get_target(words, idx, window_size=5):\n",
        "    ''' Get a list of words in a window around an index. '''\n",
        "\n",
        "    R = np.random.randint(1, window_size+1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
        "\n",
        "    return list(target_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UND1dixgN5b",
        "outputId": "fcc44afe-9941-4eeb-9048-51a23c984c65"
      },
      "source": [
        "# test your code!\n",
        "\n",
        "# run this cell multiple times to check for random window selection\n",
        "int_text = [i for i in range(10)]\n",
        "print('Input: ', int_text)\n",
        "idx=5 # word index of interest\n",
        "\n",
        "target = get_target(int_text, idx=idx, window_size=2)\n",
        "print('Target: ', target)  # you should get some indices around the idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Target:  [4, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCu9OQEaglx9"
      },
      "source": [
        "def get_batches(words, batch_size, window_size=5):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "\n",
        "    n_batches = len(words)//batch_size\n",
        "\n",
        "    # only full batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "\n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_target(batch, ii, window_size)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DbevhGdgoiO",
        "outputId": "7f5f8399-9b5a-4aaf-ccae-8b619ac319ed"
      },
      "source": [
        "int_text = [i for i in range(20)]\n",
        "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
        "\n",
        "print('x\\n', x)\n",
        "print('y\\n', y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [0, 0, 1, 1, 2, 2, 3]\n",
            "y\n",
            " [1, 2, 0, 2, 1, 3, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tePB_BJthUoz"
      },
      "source": [
        "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
        "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
        "        Here, embedding should be a PyTorch embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here we're calculating the cosine similarity between some random words and\n",
        "    # our embedding vectors. With the similarities, we can look at what words are\n",
        "    # close to our random words.\n",
        "\n",
        "    # sim = (a . b) / |a||b|\n",
        "\n",
        "    embed_vectors = embedding.weight\n",
        "\n",
        "    # magnitude of embedding vectors, |b|\n",
        "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
        "\n",
        "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent\n",
        "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
        "    valid_examples = np.append(valid_examples,\n",
        "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
        "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
        "\n",
        "    valid_vectors = embedding(valid_examples)\n",
        "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
        "\n",
        "    return valid_examples, similarities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiBXn-yNe3VT"
      },
      "source": [
        "class SkipGramNeg(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_embed = n_embed\n",
        "        self.noise_dist = noise_dist\n",
        "\n",
        "        # define embedding layers for input and output words\n",
        "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
        "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
        "\n",
        "        # Initialize both embedding tables with uniform distribution\n",
        "        self.in_embed.weight.data.uniform_(-1, 1)\n",
        "        self.out_embed.weight.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward_input(self, input_words):\n",
        "        # return input vector embeddings\n",
        "\n",
        "        return self.in_embed(input_words)\n",
        "\n",
        "    def forward_output(self, output_words):\n",
        "        # return output vector embeddings\n",
        "\n",
        "        return self.out_embed(output_words)\n",
        "\n",
        "    def forward_noise(self, batch_size, n_samples):\n",
        "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n",
        "        if self.noise_dist is None:\n",
        "            # Sample words uniformly\n",
        "            noise_dist = torch.ones(self.n_vocab)\n",
        "        else:\n",
        "            noise_dist = self.noise_dist\n",
        "\n",
        "        # Sample words from our noise distribution\n",
        "        noise_words = torch.multinomial(noise_dist,\n",
        "                                        batch_size * n_samples,\n",
        "                                        replacement=True)\n",
        "\n",
        "        device = \"cuda\" if model.out_embed.weight.is_cuda else \"cpu\"\n",
        "        noise_words = noise_words.to(device)\n",
        "\n",
        "        ## TODO: get the noise embeddings\n",
        "        # reshape the embeddings so that they have dims (batch_size, n_samples, n_embed)\n",
        "        noise_words = self.out_embed(noise_words)\n",
        "        noise_words = noise_words.view(batch_size, n_samples, self.n_embed)\n",
        "\n",
        "        return noise_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kifVOvuWe7wN"
      },
      "source": [
        "class NegativeSamplingLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
        "\n",
        "        batch_size, embed_size = input_vectors.shape\n",
        "\n",
        "        # Input vectors should be a batch of column vectors\n",
        "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
        "\n",
        "        # Output vectors should be a batch of row vectors\n",
        "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
        "\n",
        "        # bmm = batch matrix multiplication\n",
        "        # correct log-sigmoid loss\n",
        "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
        "        out_loss = out_loss.squeeze()\n",
        "\n",
        "        # incorrect log-sigmoid loss\n",
        "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
        "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
        "\n",
        "        # negate and sum correct and noisy log-sigmoid losses\n",
        "        # return average batch loss\n",
        "        return -(out_loss + noise_loss).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMRGcgp1h0-C"
      },
      "source": [
        "freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QTALUqde_EA",
        "outputId": "fa97892c-2527-4bd8-f4da-ae35e24cba61"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Get our noise distribution\n",
        "# Using word frequencies calculated earlier in the notebook\n",
        "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
        "unigram_dist = word_freqs/word_freqs.sum()\n",
        "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
        "\n",
        "# instantiating the model\n",
        "embedding_dim = 300\n",
        "model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
        "\n",
        "# using the loss that we defined\n",
        "criterion = NegativeSamplingLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "print_every = 10\n",
        "steps = 0\n",
        "epochs = 50\n",
        "\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "\n",
        "    # get our input, target batches\n",
        "    for input_words, target_words in get_batches(train_words, 2):\n",
        "        steps += 1\n",
        "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # input, outpt, and noise vectors\n",
        "        input_vectors = model.forward_input(inputs)\n",
        "        output_vectors = model.forward_output(targets)\n",
        "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
        "\n",
        "\n",
        "        # negative sampling loss\n",
        "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if steps % print_every == 0:\n",
        "            print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
        "            print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
        "            # valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device, valid_size=2)\n",
        "            # _, closest_idxs = valid_similarities.topk(6)\n",
        "\n",
        "            # valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
        "            # for ii, valid_idx in enumerate(valid_examples):\n",
        "            #     closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
        "            #     print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
        "            print(\"...\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2/50\n",
            "Loss:  19.8654727935791\n",
            "...\n",
            "\n",
            "Epoch: 4/50\n",
            "Loss:  11.68275260925293\n",
            "...\n",
            "\n",
            "Epoch: 6/50\n",
            "Loss:  6.8022141456604\n",
            "...\n",
            "\n",
            "Epoch: 8/50\n",
            "Loss:  9.818048477172852\n",
            "...\n",
            "\n",
            "Epoch: 10/50\n",
            "Loss:  11.939051628112793\n",
            "...\n",
            "\n",
            "Epoch: 12/50\n",
            "Loss:  14.844532012939453\n",
            "...\n",
            "\n",
            "Epoch: 14/50\n",
            "Loss:  8.164490699768066\n",
            "...\n",
            "\n",
            "Epoch: 16/50\n",
            "Loss:  3.639408826828003\n",
            "...\n",
            "\n",
            "Epoch: 18/50\n",
            "Loss:  5.670575141906738\n",
            "...\n",
            "\n",
            "Epoch: 20/50\n",
            "Loss:  1.9219176769256592\n",
            "...\n",
            "\n",
            "Epoch: 22/50\n",
            "Loss:  1.4990862607955933\n",
            "...\n",
            "\n",
            "Epoch: 24/50\n",
            "Loss:  0.49791812896728516\n",
            "...\n",
            "\n",
            "Epoch: 26/50\n",
            "Loss:  1.2909852266311646\n",
            "...\n",
            "\n",
            "Epoch: 28/50\n",
            "Loss:  1.5197029113769531\n",
            "...\n",
            "\n",
            "Epoch: 30/50\n",
            "Loss:  1.282673716545105\n",
            "...\n",
            "\n",
            "Epoch: 32/50\n",
            "Loss:  1.9757285118103027\n",
            "...\n",
            "\n",
            "Epoch: 34/50\n",
            "Loss:  1.1976169347763062\n",
            "...\n",
            "\n",
            "Epoch: 36/50\n",
            "Loss:  3.9572222232818604\n",
            "...\n",
            "\n",
            "Epoch: 38/50\n",
            "Loss:  4.885851860046387\n",
            "...\n",
            "\n",
            "Epoch: 40/50\n",
            "Loss:  0.3559829592704773\n",
            "...\n",
            "\n",
            "Epoch: 42/50\n",
            "Loss:  5.517121315002441\n",
            "...\n",
            "\n",
            "Epoch: 44/50\n",
            "Loss:  1.7190409898757935\n",
            "...\n",
            "\n",
            "Epoch: 46/50\n",
            "Loss:  5.399352550506592\n",
            "...\n",
            "\n",
            "Epoch: 48/50\n",
            "Loss:  0.38999342918395996\n",
            "...\n",
            "\n",
            "Epoch: 50/50\n",
            "Loss:  1.3599188327789307\n",
            "...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoNuz49Sh_ib"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}