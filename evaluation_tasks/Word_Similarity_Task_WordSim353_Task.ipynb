{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WORD Similarity Task in WordSim353\n",
        "\n",
        "A task to test word embeddings and their smeantics relations\n",
        "\n",
        "Dataset:\n",
        "Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, Aitor Soroa, A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches, In Proceedings of NAACL-HLT 2009 - at http://alfonseca.org/eng/research/wordsim353.html\n",
        "\n",
        "Reference:\n",
        "- Katrin Erk(2020) Python demo: predicting word similarity\n",
        " at https://www.katrinerk.com/courses/python-worksheets/python-demo-predicting-word-similarity\n",
        "- Gulonnlp Framework for embedding evaluation at https://nlp.gluon.ai/examples/word_embedding_evaluation/word_embedding_evaluation.html"
      ],
      "metadata": {
        "id": "2-CIAMoSfRX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas\n",
        "import scipy\n",
        "import warnings\n",
        "from gensim.models import KeyedVectors\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "g0Pfc1TW5m2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can upload files in Google Colab easily with the following comman. If you are working locally then you can ignore the following cell"
      ],
      "metadata": {
        "id": "t_kioenNfxe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the data to google cloud in case the drag-drop upload is not working.\n",
        "# This was the case on my side\n",
        "from google.colab import files\n",
        "dataset_file_dict = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "kwrB2O3p4nOg",
        "outputId": "01928047-8891-4f56-d1b9-b42b1fb1b70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2e07dc61-a1de-4661-87c9-1c20b08d02f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2e07dc61-a1de-4661-87c9-1c20b08d02f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving win353.csv to win353.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other files that are used for testing. If you do not have them no worries just ignore the following cells. This were old test and experimental results.\n",
        "\n",
        "You need to specify your own file paths accordingly."
      ],
      "metadata": {
        "id": "I9SKcMqlf7Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "large_corpus = []\n",
        "with open('/content/line_corpus.txt', 'r') as inp:\n",
        "    for line in inp:\n",
        "        large_corpus.append(line.split())\n",
        "\n",
        "small_corpus = []\n",
        "with open('/content/small_line_corpus_no_stopwords.txt', 'r') as inp:\n",
        "    for line in inp:\n",
        "        small_corpus.append(line.split())"
      ],
      "metadata": {
        "id": "9ymAbdP0z7VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we build a toy distributional space\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "brownmodel1 = Word2Vec(brown.sents(), iter=100, min_count=10, size=128, workers=4)\n",
        "# brownmodel2 = Word2Vec(brown.sents(), iter=100, min_count=10, size=100, workers=4)"
      ],
      "metadata": {
        "id": "qnHku1-4P4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some other models just for testing with our corpus\n",
        "word2vec_small = Word2Vec(small_corpus, iter=100, min_count=10, size=128, workers=4)\n",
        "word2vec_big = Word2Vec(large_corpus, iter=100, min_count=10, size=128, workers=4)"
      ],
      "metadata": {
        "id": "HXWhMkQUqOmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since some files are really large to reupload them every time I have already uploaded them in drive and used from my drive. You can ignore the drive setup if you are working locally. Just make sure to chage the path of the files accordingly."
      ],
      "metadata": {
        "id": "sOPN_4IMgOxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi3wGqHDYzFw",
        "outputId": "696a68ce-c123-4bda-dc15-a142be155efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is from my drive\n",
        "enahnced_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_128_enhanced_ws2.tsv\"\n",
        "normal_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_128_normal_ws2.tsv\"\n",
        "enahnced_file_ws3 = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_128_enhanced_ws3.tsv\"\n",
        "normal_file_ws3 = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_128_normal_ws3.tsv\"\n",
        "\n",
        "glove_file = \"/content/gdrive/MyDrive/GloveEmbeddings/glove.twitter.27B.200d.txt\""
      ],
      "metadata": {
        "id": "VQPtq_TXbLCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_wg2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_50_enhanced_200k.tsv\"\n",
        "original_wg2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_50_enhanced_200k_small.tsv\"\n",
        "loss_wg2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_original_graph_wg2v.tsv\"\n",
        "baseline_w2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_50_normal_big.tsv\"\n",
        "synonym_w2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_synonym_relation_w2v.tsv\"\n",
        "hypernym_w2v_file = \"/content/gdrive/MyDrive/GloveEmbeddings/embeddings_hypernym_relation_w2v.tsv\""
      ],
      "metadata": {
        "id": "euS_0OuNNC1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the file in correct format. For each file  that you want to test please specify the path and the name you want the file to be saved. Also count the number of words that are supposed to be in the file."
      ],
      "metadata": {
        "id": "pwEkGi4_hQjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function that converts TSV embedding files into .txt files used by gensim library."
      ],
      "metadata": {
        "id": "gglTSQRR2BSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writeEmbeddings2File(file, title='word2vec-enhanced-format.txt', line_count='60400', dimensions='100'):\n",
        "    with open(file, 'r') as inp, open('/content/'+title, 'w') as outp:\n",
        "        # The genism library needs the fist line of text to be line_count dimensions\n",
        "        # There is a wrong number\n",
        "        outp.write(' '.join([line_count, dimensions]) + '\\n')\n",
        "        for line in inp:\n",
        "            if '60401 128' in line:\n",
        "                print(line)\n",
        "                continue\n",
        "            words = line.strip().split()\n",
        "            outp.write(' '.join(words) + '\\n')"
      ],
      "metadata": {
        "id": "Zo63GU7Ggelb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writeEmbeddings2File(improved_wg2v_file, 'wg2v-improved.txt', '282164', '50')\n",
        "writeEmbeddings2File(original_wg2v_file, 'wg2v-original.txt', '131110', '50')\n",
        "writeEmbeddings2File(loss_wg2v_file, 'wg2v-loss.txt', '253696')\n",
        "writeEmbeddings2File(baseline_w2v_file, 'w2v-baseline.txt', '282164', '50')\n",
        "writeEmbeddings2File(synonym_w2v_file, 'w2v-synonym.txt', '253696')\n",
        "writeEmbeddings2File(hypernym_w2v_file, 'w2v-hypernym.txt', '253696')"
      ],
      "metadata": {
        "id": "RTJRFSl6M5El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordgraph2vec_improved = KeyedVectors.load_word2vec_format('/content/wg2v-improved.txt')\n",
        "wordgraph2vec_original = KeyedVectors.load_word2vec_format('/content/wg2v-original.txt')\n",
        "wordgraph2vec_loss = KeyedVectors.load_word2vec_format('/content/wg2v-loss.txt')\n",
        "word2vec_baseline = KeyedVectors.load_word2vec_format('/content/w2v-baseline.txt')\n",
        "word2vec_synonym = KeyedVectors.load_word2vec_format('/content/w2v-synonym.txt')\n",
        "word2vec_hypernym = KeyedVectors.load_word2vec_format('/content/w2v-hypernym.txt')"
      ],
      "metadata": {
        "id": "gpEqOsATM8Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_enhanced_ws2 = KeyedVectors.load_word2vec_format('/content/word2vec-enhanced-format.txt')\n",
        "word2vec_model_normal_ws2 = KeyedVectors.load_word2vec_format('/content/word2vec-normal-format.txt')"
      ],
      "metadata": {
        "id": "lbU5rZiVY9Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_enhanced_ws3 = KeyedVectors.load_word2vec_format('/content/word2vec-enhanced-ws3-format.txt')\n",
        "word2vec_model_normal_ws3 = KeyedVectors.load_word2vec_format('/content/word2vec-normal-ws3-format.txt')"
      ],
      "metadata": {
        "id": "RLpiMsLeX3XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model = KeyedVectors.load_word2vec_format(glove_file)"
      ],
      "metadata": {
        "id": "kosVez06sluH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Enhanced Embeddings:', word2vec_model_enhanced.similarity('write', 'writes'))\n",
        "print('Normal Embeddings:', word2vec_model_normal.similarity('write', 'writes'))\n",
        "print('Word2Vec Embeddings:', brownmodel1.similarity('write', 'writes'))\n",
        "print('Glove Embeddings:', glove_model.similarity('write', 'writes'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK70BgP2hAYu",
        "outputId": "76a132b4-8f72-471e-c033-c95464fe953d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Embeddings: 0.23435295\n",
            "Normal Embeddings: 0.14642885\n",
            "Word2Vec Embeddings: 0.22172073\n",
            "Glove Embeddings: 0.6877123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the word similarity dataset"
      ],
      "metadata": {
        "id": "zueNLzbehigl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read wordsim353. separating two columns is whitespace\n",
        "filename = \"/content/win353.csv\"\n",
        "wordsim353 = pandas.read_csv(filename)"
      ],
      "metadata": {
        "id": "Gd9b3m_jPy0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-JYEPocyvhd",
        "outputId": "54517052-4d9c-4f25-b7ce-532c3f3482d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Word 1      Word 2  Human (Mean)\n",
            "0       admission      ticket         5.846\n",
            "1         alcohol   chemistry         1.154\n",
            "2        aluminum       metal         6.286\n",
            "3    announcement      effort         2.000\n",
            "4    announcement        news         7.077\n",
            "..            ...         ...           ...\n",
            "348        weapon      secret         1.500\n",
            "349       weather    forecast         5.067\n",
            "350     Wednesday        news         1.000\n",
            "351          wood      forest         7.214\n",
            "352          word  similarity         0.923\n",
            "\n",
            "[353 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# accessing this table\n",
        "print(wordsim353)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pulling similarity ratings from the model:\n",
        "# if a word is missing, we want to just return a similarity of zero\n",
        "# If fall back is provided word is used with that model\n",
        "def sim_or_zero(word1, word2, model, fallBack):\n",
        "    if model.wv.__contains__(word1) and model.wv.__contains__(word2):\n",
        "        # Return the similarity score between words\n",
        "        return model.wv.similarity(word1, word2), _\n",
        "    else:\n",
        "        # Try the fall back model in case a word is missing\n",
        "        if fallBack.wv.__contains__(word1) and fallBack.wv.__contains__(word2):\n",
        "            return fallBack.wv.similarity(word1, word2), _\n",
        "        else:\n",
        "            return 0.0, True"
      ],
      "metadata": {
        "id": "O2FLiJb9P9gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordgraph2vec_improved = KeyedVectors.load_word2vec_format('/content/wg2v-improved.txt')\n",
        "# wordgraph2vec_original = KeyedVectors.load_word2vec_format('/content/wg2v-original.txt')\n",
        "# wordgraph2vec_loss = KeyedVectors.load_word2vec_format('/content/wg2v-loss.txt')\n",
        "# word2vec_baseline = KeyedVectors.load_word2vec_format('/content/w2v-baseline.txt')\n",
        "# word2vec_synonym = KeyedVectors.load_word2vec_format('/content/w2v-synonym.txt')\n",
        "# word2vec_hypernym = KeyedVectors.load_word2vec_format('/content/w2v-hypernym.txt')\n",
        "\n",
        "# making predictions for the wordsim353 data,\n",
        "# storing them in the column \"modelpredict\"\n",
        "modelpredict_wordgraph2vec_improved = [ ]\n",
        "modelpredict_wordgraph2vec_original = [ ]\n",
        "modelpredict_wordgraph2vec_loss = [ ]\n",
        "modelpredict_word2vec_baseline = [ ]\n",
        "modelpredict_word2vec_synonym = [ ]\n",
        "modelpredict_word2vec_hypernym = [ ]\n",
        "glovePred = []\n",
        "word2vecPred = []\n",
        "for index, row in wordsim353.iterrows():\n",
        "    modelpredict_wordgraph2vec_improved.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], wordgraph2vec_improved, glove_model) )\n",
        "    modelpredict_wordgraph2vec_original.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], wordgraph2vec_original, glove_model) )\n",
        "    modelpredict_wordgraph2vec_loss.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], wordgraph2vec_loss, glove_model) )\n",
        "    modelpredict_word2vec_baseline.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_baseline, glove_model) )\n",
        "    modelpredict_word2vec_synonym.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_synonym, glove_model) )\n",
        "    modelpredict_word2vec_hypernym.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_hypernym, glove_model) )\n",
        "\n",
        "wordsim353[\"modelpredict_wordgraph2vec_improved\"] = modelpredict_wordgraph2vec_improved\n",
        "wordsim353[\"modelpredict_wordgraph2vec_original\"] = modelpredict_wordgraph2vec_original\n",
        "wordsim353[\"modelpredict_wordgraph2vec_loss\"] = modelpredict_wordgraph2vec_loss\n",
        "wordsim353[\"modelpredict_word2vec_baseline\"] = modelpredict_word2vec_baseline\n",
        "wordsim353[\"modelpredict_word2vec_synonym\"] = modelpredict_word2vec_synonym\n",
        "wordsim353[\"modelpredict_word2vec_hypernym\"] = modelpredict_word2vec_hypernym\n",
        "\n",
        "# we print pairs of correlation and pvalue\n",
        "\n",
        "print('modelpredict_wordgraph2vec_improved Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_wordgraph2vec_improved\"]), '\\n')\n",
        "\n",
        "print('modelpredict_wordgraph2vec_original Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_wordgraph2vec_original\"]), '\\n')\n",
        "\n",
        "print('modelpredict_wordgraph2vec_loss Embedding Result 128 WS=3')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_wordgraph2vec_loss\"]), '\\n')\n",
        "\n",
        "print('modelpredict_word2vec_baseline Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_word2vec_baseline\"]), '\\n')\n",
        "\n",
        "print('modelpredict_word2vec_synonym Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_word2vec_synonym\"]), '\\n')\n",
        "\n",
        "print('modelpredict_word2vec_hypernym Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredict_word2vec_hypernym\"]), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwh2ZqpAOh5S",
        "outputId": "0b589d04-6008-4752-f8e6-bf91123dc9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelpredict_wordgraph2vec_improved Embedding Result\n",
            "SpearmanrResult(correlation=0.29723642045511517, pvalue=1.241287694374442e-08) \n",
            "\n",
            "modelpredict_wordgraph2vec_original Embedding Result\n",
            "SpearmanrResult(correlation=0.2617145274063511, pvalue=6.135095460463393e-07) \n",
            "\n",
            "modelpredict_wordgraph2vec_loss Embedding Result 128 WS=3\n",
            "SpearmanrResult(correlation=-0.040117485646638654, pvalue=0.45243106805183453) \n",
            "\n",
            "modelpredict_word2vec_baseline Embedding Result\n",
            "SpearmanrResult(correlation=0.3725227252103845, pvalue=4.609207445689924e-13) \n",
            "\n",
            "modelpredict_word2vec_synonym Embedding Result\n",
            "SpearmanrResult(correlation=-0.051021877478921354, pvalue=0.33915476269219547) \n",
            "\n",
            "modelpredict_word2vec_hypernym Embedding Result\n",
            "SpearmanrResult(correlation=-0.0355718063161641, pvalue=0.5052999272219449) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old Experimental Results"
      ],
      "metadata": {
        "id": "GdOyyrtxOfiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions for the wordsim353 data,\n",
        "# storing them in the column \"modelpredict\"\n",
        "# as one-liners:\n",
        "# wordsim353[\"modelpredict1\"] = [sim_or_zero(row[\"Word1\"], row[\"Word2\"], brownmodel1) for index, row in wordsim353.iterrows()]\n",
        "# wordsim353[\"modelpredict2\"] = [sim_or_zero(row[\"Word1\"], row[\"Word2\"], brownmodel2) for index, row in wordsim353.iterrows()]\n",
        "# or less compactly:\n",
        "modelpredictEnhancedws2 = [ ]\n",
        "modelpredictNormalws2 = [ ]\n",
        "modelpredictEnhancedws3 = [ ]\n",
        "modelpredictNormalws3 = [ ]\n",
        "modelpredictSmall = [ ]\n",
        "modelpredictBig = [ ]\n",
        "glovePred = []\n",
        "word2vecPred = []\n",
        "for index, row in wordsim353.iterrows():\n",
        "    modelpredictEnhancedws2.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_model_enhanced_ws2, glove_model) )\n",
        "    modelpredictNormalws2.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_model_normal_ws2, glove_model) )\n",
        "    modelpredictEnhancedws3.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_model_enhanced_ws3, glove_model) )\n",
        "    modelpredictNormalws3.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_model_normal_ws3, glove_model) )\n",
        "    word2vecPred.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], brownmodel1, glove_model) )\n",
        "    modelpredictSmall.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_small, brownmodel1) )\n",
        "    modelpredictBig.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], word2vec_big, brownmodel1) )\n",
        "    glovePred.append( sim_or_zero(row[\"Word 1\"], row[\"Word 2\"], glove_model, brownmodel1) )\n",
        "\n",
        "\n",
        "wordsim353[\"modelpredictEnhancedws2\"] = modelpredictEnhancedws2\n",
        "wordsim353[\"modelpredictNormalws2\"] = modelpredictNormalws2\n",
        "wordsim353[\"modelpredictEnhancedws3\"] = modelpredictEnhancedws3\n",
        "wordsim353[\"modelpredictNormalws3\"] = modelpredictNormalws3\n",
        "wordsim353[\"modelpredictSmall\"] = modelpredictSmall\n",
        "wordsim353[\"modelpredictBig\"] = modelpredictBig\n",
        "wordsim353[\"glovePred\"] = glovePred\n",
        "wordsim353[\"word2VecPred\"] = word2vecPred\n",
        "\n",
        "\n",
        "# we print pairs of correlation and pvalue\n",
        "# brownmodel1 is beyond miserable\n",
        "print('Enhanced Embedding Result 128 WS=2')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictEnhancedws2\"]), '\\n')\n",
        "\n",
        "# brownmodel2 has looked at the data a greater number of times,\n",
        "# and compresses its information into fewer dimensions.\n",
        "# It does better, though also not great.\n",
        "# the words with missing entries are really harming the model\n",
        "print('Normal Embedding Result 128 WS=2')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictNormalws2\"]), '\\n')\n",
        "\n",
        "\n",
        "print('Enhanced Embedding Result 128 WS=3')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictEnhancedws3\"]), '\\n')\n",
        "\n",
        "# brownmodel2 has looked at the data a greater number of times,\n",
        "# and compresses its information into fewer dimensions.\n",
        "# It does better, though also not great.\n",
        "# the words with missing entries are really harming the model\n",
        "print('Normal Embedding Result 128 WS=3')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictNormalws3\"]), '\\n')\n",
        "\n",
        "print('Big Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictBig\"]), '\\n')\n",
        "\n",
        "print('Small Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"modelpredictSmall\"]), '\\n')\n",
        "\n",
        "print('Glove Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"glovePred\"]), '\\n')\n",
        "\n",
        "print('Word2vec Brown Embedding Result')\n",
        "print(scipy.stats.spearmanr(wordsim353[\"Human (Mean)\"], wordsim353[\"word2VecPred\"]), '\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltmHxXKdP71R",
        "outputId": "9a31b8ed-2184-4a57-d328-c8da299f30e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced Embedding Result 128 WS=2\n",
            "SpearmanrResult(correlation=0.20220446404371606, pvalue=0.0001306722734620016) \n",
            "\n",
            "Normal Embedding Result 128 WS=2\n",
            "SpearmanrResult(correlation=0.14972834204305163, pvalue=0.004816292922489155) \n",
            "\n",
            "Enhanced Embedding Result 128 WS=3\n",
            "SpearmanrResult(correlation=0.24398752181175862, pvalue=3.5157224443736937e-06) \n",
            "\n",
            "Normal Embedding Result 128 WS=3\n",
            "SpearmanrResult(correlation=0.15725722404236184, pvalue=0.003050827369036314) \n",
            "\n",
            "Big Embedding Result\n",
            "SpearmanrResult(correlation=0.4892507259510937, pvalue=1.207630283546632e-22) \n",
            "\n",
            "Small Embedding Result\n",
            "SpearmanrResult(correlation=0.15299156959054352, pvalue=0.003961191741279982) \n",
            "\n",
            "Glove Embedding Result\n",
            "SpearmanrResult(correlation=0.38880933943525936, pvalue=3.4704034981132106e-14) \n",
            "\n",
            "Word2vec Brown Embedding Result\n",
            "SpearmanrResult(correlation=0.37429903459685054, pvalue=3.500613751849894e-13) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}